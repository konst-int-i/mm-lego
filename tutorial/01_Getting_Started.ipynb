{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d8b59f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mm_lego.models.lego import LegoBlock\n",
    "from mm_lego.models import LegoBlock, MILAttentionNet, SNN, LegoMerge, LegoFuse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871b945",
   "metadata": {},
   "source": [
    "## Wrapping Encoders in LegoBlocks\n",
    "\n",
    "One of MM-Lego's key components is the LegoBlock - you can use your initial unimodal architecture and fit a LegoBlock around it. Let's assume a case where you have a tabular and and imaging modality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd0f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 10 # batch size\n",
    "# Note - dimensions always denoted as\n",
    "t_c = 1  # number of channels (1 for tabular) ; note that channels correspond to modality input/features\n",
    "t_d = 2189  # dimensions of each channel\n",
    "i_c = 100  # >1 if using MIL setup\n",
    "i_d = 1024  # dimensions per patch\n",
    "latent = torch.randn(b, 256, 32)\n",
    "\n",
    "tab_data = torch.randn(b, t_c, t_d)  # expects (b dims channels)\n",
    "img_data = torch.randn(b, i_c, i_d)\n",
    "\n",
    "tab_enc = SNN(t_d, final_head=False)\n",
    "img_enc = MILAttentionNet(torch.Size((i_c, i_d)), final_head=False, size_arg=\"tcga\")\n",
    "\n",
    "# Lego Wrapper\n",
    "tab_block = LegoBlock(in_shape=(t_c, t_d), encoder=tab_enc)\n",
    "img_block = LegoBlock(in_shape=(i_c, i_d), encoder=img_enc)\n",
    "\n",
    "# Forward pass of block\n",
    "print(img_block([img_data], return_embeddings=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145690b5",
   "metadata": {},
   "source": [
    "## Merging & Fusing Blocks\n",
    "\n",
    "After fitting each unimodal block, we can merge them into a multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05441b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = LegoMerge(blocks=[tab_block, img_block], head_method=\"slerp\", final_head=False)\n",
    "\n",
    "# forward pass of merged model\n",
    "merged_model([tab_data, img_data], return_embeddings=True)\n",
    "\n",
    "fusion_model = LegoFuse(blocks=[tab_block, img_block], fuse_method=\"stack\", head_method=\"slerp\", final_head=False)\n",
    "\n",
    "# Forward pass of fusion model\n",
    "fusion_model([tab_data, img_data], return_embeddings=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
